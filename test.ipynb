{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 00:31:32.982699: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734280292.994441   85899 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734280292.997888   85899 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-16 00:31:33.010884: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import time\n",
    "import spinup\n",
    "import spinup.algos.tf1.trpo.core as core\n",
    "from spinup.utils.logx import EpochLogger\n",
    "from spinup.utils.mpi_tf import MpiAdamOptimizer, sync_all_params\n",
    "from spinup.utils.mpi_tools import mpi_fork, mpi_avg, proc_id, mpi_statistics_scalar, num_procs\n",
    "\n",
    "\n",
    "EPS = 1e-8\n",
    "\n",
    "class GAEBuffer:\n",
    "    \"\"\"\n",
    "    A buffer for storing trajectories experienced by a TRPO agent interacting\n",
    "    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
    "    for calculating the advantages of state-action pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size, info_shapes, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = np.zeros(core.combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(core.combined_shape(size, act_dim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.info_bufs = {k: np.zeros([size] + list(v), dtype=np.float32) for k,v in info_shapes.items()}\n",
    "        self.sorted_info_keys = core.keys_as_sorted_list(self.info_bufs)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, val, logp, info):\n",
    "        \"\"\"\n",
    "        Append one timestep of agent-environment interaction to the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr < self.max_size     # buffer has to have room so you can store\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        for i, k in enumerate(self.sorted_info_keys):\n",
    "            self.info_bufs[k][self.ptr] = info[i]\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        \"\"\"\n",
    "        Call this at the end of a trajectory, or when one gets cut off\n",
    "        by an epoch ending. This looks back in the buffer to where the\n",
    "        trajectory started, and uses rewards and value estimates from\n",
    "        the whole trajectory to compute advantage estimates with GAE-Lambda,\n",
    "        as well as compute the rewards-to-go for each state, to use as\n",
    "        the targets for the value function.\n",
    "\n",
    "        The \"last_val\" argument should be 0 if the trajectory ended\n",
    "        because the agent reached a terminal state (died), and otherwise\n",
    "        should be V(s_T), the value function estimated for the last state.\n",
    "        This allows us to bootstrap the reward-to-go calculation to account\n",
    "        for timesteps beyond the arbitrary episode horizon (or epoch cutoff).\n",
    "        \"\"\"\n",
    "\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "        \n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = core.discount_cumsum(deltas, self.gamma * self.lam)\n",
    "        \n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf[path_slice] = core.discount_cumsum(rews, self.gamma)[:-1]\n",
    "        \n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        Call this at the end of an epoch to get all of the data from\n",
    "        the buffer, with advantages appropriately normalized (shifted to have\n",
    "        mean zero and std one). Also, resets some pointers in the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr == self.max_size    # buffer has to be full before you can get\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "        # the next two lines implement the advantage normalization trick\n",
    "        adv_mean, adv_std = mpi_statistics_scalar(self.adv_buf)\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
    "        return [self.obs_buf, self.act_buf, self.adv_buf, self.ret_buf, \n",
    "                self.logp_buf] + core.values_as_sorted_list(self.info_bufs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trpo(env_fn, actor_critic=core.mlp_actor_critic, ac_kwargs=dict(), seed=0, \n",
    "         steps_per_epoch=4000, epochs=50, gamma=0.99, delta=0.01, vf_lr=1e-3,\n",
    "         train_v_iters=80, damping_coeff=0.1, cg_iters=10, backtrack_iters=10, \n",
    "         backtrack_coeff=0.8, lam=0.97, max_ep_len=1000, logger_kwargs=dict(), \n",
    "         save_freq=10, algo='trpo'):\n",
    "    \"\"\"\n",
    "    Trust Region Policy Optimization \n",
    "\n",
    "    (with support for Natural Policy Gradient)\n",
    "\n",
    "    Args:\n",
    "        env_fn : A function which creates a copy of the environment.\n",
    "            The environment must satisfy the OpenAI Gym API.\n",
    "\n",
    "        actor_critic: A function which takes in placeholder symbols \n",
    "            for state, ``x_ph``, and action, ``a_ph``, and returns the main \n",
    "            outputs from the agent's Tensorflow computation graph:\n",
    "\n",
    "            ============  ================  ========================================\n",
    "            Symbol        Shape             Description\n",
    "            ============  ================  ========================================\n",
    "            ``pi``        (batch, act_dim)  | Samples actions from policy given \n",
    "                                            | states.\n",
    "            ``logp``      (batch,)          | Gives log probability, according to\n",
    "                                            | the policy, of taking actions ``a_ph``\n",
    "                                            | in states ``x_ph``.\n",
    "            ``logp_pi``   (batch,)          | Gives log probability, according to\n",
    "                                            | the policy, of the action sampled by\n",
    "                                            | ``pi``.\n",
    "            ``info``      N/A               | A dict of any intermediate quantities\n",
    "                                            | (from calculating the policy or log \n",
    "                                            | probabilities) which are needed for\n",
    "                                            | analytically computing KL divergence.\n",
    "                                            | (eg sufficient statistics of the\n",
    "                                            | distributions)\n",
    "            ``info_phs``  N/A               | A dict of placeholders for old values\n",
    "                                            | of the entries in ``info``.\n",
    "            ``d_kl``      ()                | A symbol for computing the mean KL\n",
    "                                            | divergence between the current policy\n",
    "                                            | (``pi``) and the old policy (as \n",
    "                                            | specified by the inputs to \n",
    "                                            | ``info_phs``) over the batch of \n",
    "                                            | states given in ``x_ph``.\n",
    "            ``v``         (batch,)          | Gives the value estimate for states\n",
    "                                            | in ``x_ph``. (Critical: make sure \n",
    "                                            | to flatten this!)\n",
    "            ============  ================  ========================================\n",
    "\n",
    "        ac_kwargs (dict): Any kwargs appropriate for the actor_critic \n",
    "            function you provided to TRPO.\n",
    "\n",
    "        seed (int): Seed for random number generators.\n",
    "\n",
    "        steps_per_epoch (int): Number of steps of interaction (state-action pairs) \n",
    "            for the agent and the environment in each epoch.\n",
    "\n",
    "        epochs (int): Number of epochs of interaction (equivalent to\n",
    "            number of policy updates) to perform.\n",
    "\n",
    "        gamma (float): Discount factor. (Always between 0 and 1.)\n",
    "\n",
    "        delta (float): KL-divergence limit for TRPO / NPG update. \n",
    "            (Should be small for stability. Values like 0.01, 0.05.)\n",
    "\n",
    "        vf_lr (float): Learning rate for value function optimizer.\n",
    "\n",
    "        train_v_iters (int): Number of gradient descent steps to take on \n",
    "            value function per epoch.\n",
    "\n",
    "        damping_coeff (float): Artifact for numerical stability, should be \n",
    "            smallish. Adjusts Hessian-vector product calculation:\n",
    "            \n",
    "            .. math:: Hv \\\\rightarrow (\\\\alpha I + H)v\n",
    "\n",
    "            where :math:`\\\\alpha` is the damping coefficient. \n",
    "            Probably don't play with this hyperparameter.\n",
    "\n",
    "        cg_iters (int): Number of iterations of conjugate gradient to perform. \n",
    "            Increasing this will lead to a more accurate approximation\n",
    "            to :math:`H^{-1} g`, and possibly slightly-improved performance,\n",
    "            but at the cost of slowing things down. \n",
    "\n",
    "            Also probably don't play with this hyperparameter.\n",
    "\n",
    "        backtrack_iters (int): Maximum number of steps allowed in the \n",
    "            backtracking line search. Since the line search usually doesn't \n",
    "            backtrack, and usually only steps back once when it does, this\n",
    "            hyperparameter doesn't often matter.\n",
    "\n",
    "        backtrack_coeff (float): How far back to step during backtracking line\n",
    "            search. (Always between 0 and 1, usually above 0.5.)\n",
    "\n",
    "        lam (float): Lambda for GAE-Lambda. (Always between 0 and 1,\n",
    "            close to 1.)\n",
    "\n",
    "        max_ep_len (int): Maximum length of trajectory / episode / rollout.\n",
    "\n",
    "        logger_kwargs (dict): Keyword args for EpochLogger.\n",
    "\n",
    "        save_freq (int): How often (in terms of gap between epochs) to save\n",
    "            the current policy and value function.\n",
    "\n",
    "        algo: Either 'trpo' or 'npg': this code supports both, since they are \n",
    "            almost the same.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    logger = EpochLogger(**logger_kwargs)\n",
    "    logger.save_config(locals()) \n",
    "\n",
    "    seed += 10000 * proc_id()\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    env = env_fn()\n",
    "    obs_dim = env.observation_space.shape\n",
    "    act_dim = env.action_space.shape\n",
    "    \n",
    "    # Share information about action space with policy architecture\n",
    "    ac_kwargs['action_space'] = env.action_space\n",
    "\n",
    "    # Inputs to computation graph\n",
    "    x_ph, a_ph = core.placeholders_from_spaces(env.observation_space, env.action_space)\n",
    "    adv_ph, ret_ph, logp_old_ph = core.placeholders(None, None, None)   # advantage, ret=?;\n",
    "\n",
    "    # Main outputs from computation graph, plus placeholders for old pdist (for KL)\n",
    "    pi, logp, logp_pi, info, info_phs, d_kl, v = actor_critic(x_ph, a_ph, **ac_kwargs)\n",
    "\n",
    "    # Need all placeholders in *this* order later (to zip with data from buffer)\n",
    "    all_phs = [x_ph, a_ph, adv_ph, ret_ph, logp_old_ph] + core.values_as_sorted_list(info_phs)\n",
    "\n",
    "    # Every step, get: action, value, logprob, & info for pdist (for computing kl div)\n",
    "    get_action_ops = [pi, v, logp_pi] + core.values_as_sorted_list(info)\n",
    "\n",
    "    # Experience buffer\n",
    "    local_steps_per_epoch = int(steps_per_epoch / num_procs())\n",
    "    info_shapes = {k: v.shape.as_list()[1:] for k,v in info_phs.items()}\n",
    "    buf = GAEBuffer(obs_dim, act_dim, local_steps_per_epoch, info_shapes, gamma, lam)\n",
    "\n",
    "    # Count variables\n",
    "    var_counts = tuple(core.count_vars(scope) for scope in ['pi', 'v'])\n",
    "    logger.log('\\nNumber of parameters: \\t pi: %d, \\t v: %d\\n'%var_counts)\n",
    "\n",
    "    # TRPO losses\n",
    "    ratio = tf.exp(logp - logp_old_ph)          # pi(a|s) / pi_old(a|s)\n",
    "    pi_loss = -tf.reduce_mean(ratio * adv_ph)\n",
    "    v_loss = tf.reduce_mean((ret_ph - v)**2)\n",
    "\n",
    "    # Optimizer for value function\n",
    "    train_vf = MpiAdamOptimizer(learning_rate=vf_lr).minimize(v_loss)\n",
    "\n",
    "    # Symbols needed for CG solver\n",
    "    pi_params = core.get_vars('pi')\n",
    "    gradient = core.flat_grad(pi_loss, pi_params)\n",
    "    v_ph, hvp = core.hessian_vector_product(d_kl, pi_params)\n",
    "    if damping_coeff > 0:\n",
    "        hvp += damping_coeff * v_ph\n",
    "\n",
    "    # Symbols for getting and setting params\n",
    "    get_pi_params = core.flat_concat(pi_params)\n",
    "    set_pi_params = core.assign_params_from_flat(v_ph, pi_params)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Sync params across processes\n",
    "    sess.run(sync_all_params())\n",
    "\n",
    "    # Setup model saving\n",
    "    logger.setup_tf_saver(sess, inputs={'x': x_ph}, outputs={'pi': pi, 'v': v})\n",
    "\n",
    "    def cg(Ax, b):\n",
    "        \"\"\"\n",
    "        Conjugate gradient algorithm\n",
    "        (see https://en.wikipedia.org/wiki/Conjugate_gradient_method)\n",
    "        \"\"\"\n",
    "        x = np.zeros_like(b)\n",
    "        r = b.copy() # Note: should be 'b - Ax(x)', but for x=0, Ax(x)=0. Change if doing warm start.\n",
    "        p = r.copy()\n",
    "        r_dot_old = np.dot(r,r)\n",
    "        for _ in range(cg_iters):\n",
    "            z = Ax(p)\n",
    "            alpha = r_dot_old / (np.dot(p, z) + EPS)\n",
    "            x += alpha * p\n",
    "            r -= alpha * z\n",
    "            r_dot_new = np.dot(r,r)\n",
    "            p = r + (r_dot_new / r_dot_old) * p\n",
    "            r_dot_old = r_dot_new\n",
    "        return x\n",
    "\n",
    "    def update():\n",
    "        # Prepare hessian func, gradient eval\n",
    "        inputs = {k:v for k,v in zip(all_phs, buf.get())}\n",
    "        Hx = lambda x : mpi_avg(sess.run(hvp, feed_dict={**inputs, v_ph: x}))\n",
    "        g, pi_l_old, v_l_old = sess.run([gradient, pi_loss, v_loss], feed_dict=inputs)\n",
    "        g, pi_l_old = mpi_avg(g), mpi_avg(pi_l_old)\n",
    "\n",
    "        # Core calculations for TRPO or NPG\n",
    "        x = cg(Hx, g)\n",
    "        alpha = np.sqrt(2*delta/(np.dot(x, Hx(x))+EPS))\n",
    "        old_params = sess.run(get_pi_params)\n",
    "\n",
    "        def set_and_eval(step):\n",
    "            sess.run(set_pi_params, feed_dict={v_ph: old_params - alpha * x * step})\n",
    "            return mpi_avg(sess.run([d_kl, pi_loss], feed_dict=inputs))\n",
    "\n",
    "        if algo=='npg':\n",
    "            # npg has no backtracking or hard kl constraint enforcement\n",
    "            kl, pi_l_new = set_and_eval(step=1.)\n",
    "\n",
    "        elif algo=='trpo':\n",
    "            # trpo augments npg with backtracking line search, hard kl\n",
    "            for j in range(backtrack_iters):\n",
    "                kl, pi_l_new = set_and_eval(step=backtrack_coeff**j)\n",
    "                if kl <= delta and pi_l_new <= pi_l_old:\n",
    "                    logger.log('Accepting new params at step %d of line search.'%j)\n",
    "                    logger.store(BacktrackIters=j)\n",
    "                    break\n",
    "\n",
    "                if j==backtrack_iters-1:\n",
    "                    logger.log('Line search failed! Keeping old params.')\n",
    "                    logger.store(BacktrackIters=j)\n",
    "                    kl, pi_l_new = set_and_eval(step=0.)\n",
    "\n",
    "        # Value function updates\n",
    "        for _ in range(train_v_iters):\n",
    "            sess.run(train_vf, feed_dict=inputs)\n",
    "        v_l_new = sess.run(v_loss, feed_dict=inputs)\n",
    "\n",
    "        # Log changes from update\n",
    "        logger.store(LossPi=pi_l_old, LossV=v_l_old, KL=kl,\n",
    "                     DeltaLossPi=(pi_l_new - pi_l_old),\n",
    "                     DeltaLossV=(v_l_new - v_l_old))\n",
    "\n",
    "    start_time = time.time()\n",
    "    o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "    # Main loop: collect experience in env and update/log each epoch\n",
    "    for epoch in range(epochs):\n",
    "        for t in range(local_steps_per_epoch):\n",
    "            agent_outs = sess.run(get_action_ops, feed_dict={x_ph: o.reshape(1,-1)})\n",
    "            a, v_t, logp_t, info_t = agent_outs[0][0], agent_outs[1], agent_outs[2], agent_outs[3:]\n",
    "\n",
    "            o2, r, d, _ = env.step(a)\n",
    "            ep_ret += r\n",
    "            ep_len += 1\n",
    "\n",
    "            # save and log\n",
    "            buf.store(o, a, r, v_t, logp_t, info_t)\n",
    "            logger.store(VVals=v_t)\n",
    "\n",
    "            # Update obs (critical!)\n",
    "            o = o2\n",
    "\n",
    "            terminal = d or (ep_len == max_ep_len)\n",
    "            if terminal or (t==local_steps_per_epoch-1):\n",
    "                if not(terminal):\n",
    "                    print('Warning: trajectory cut off by epoch at %d steps.'%ep_len)\n",
    "                # if trajectory didn't reach terminal state, bootstrap value target\n",
    "                last_val = 0 if d else sess.run(v, feed_dict={x_ph: o.reshape(1,-1)})\n",
    "                buf.finish_path(last_val)\n",
    "                if terminal:\n",
    "                    # only save EpRet / EpLen if trajectory finished\n",
    "                    logger.store(EpRet=ep_ret, EpLen=ep_len)\n",
    "                o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "        # Save model\n",
    "        if (epoch % save_freq == 0) or (epoch == epochs-1):\n",
    "            logger.save_state({'env': env}, None)\n",
    "\n",
    "        # Perform TRPO or NPG update!\n",
    "        update()\n",
    "\n",
    "        # Log info about epoch\n",
    "        logger.log_tabular('Epoch', epoch)\n",
    "        logger.log_tabular('EpRet', with_min_and_max=True)\n",
    "        logger.log_tabular('EpLen', average_only=True)\n",
    "        logger.log_tabular('VVals', with_min_and_max=True)\n",
    "        logger.log_tabular('TotalEnvInteracts', (epoch+1)*steps_per_epoch)\n",
    "        logger.log_tabular('LossPi', average_only=True)\n",
    "        logger.log_tabular('LossV', average_only=True)\n",
    "        logger.log_tabular('DeltaLossPi', average_only=True)\n",
    "        logger.log_tabular('DeltaLossV', average_only=True)\n",
    "        logger.log_tabular('KL', average_only=True)\n",
    "        if algo=='trpo':\n",
    "            logger.log_tabular('BacktrackIters', average_only=True)\n",
    "        logger.log_tabular('Time', time.time()-start_time)\n",
    "        logger.dump_tabular()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Log dir /home/mike_1102/spinningup/spinningup/data/trpo/trpo_s0 already exists! Storing info there anyway.\n",
      "\u001b[32;1mLogging data to /home/mike_1102/spinningup/spinningup/data/trpo/trpo_s0/progress.txt\u001b[0m\n",
      "\u001b[36;1mSaving config:\n",
      "\u001b[0m\n",
      "{\n",
      "    \"ac_kwargs\":\t{\n",
      "        \"hidden_sizes\":\t[\n",
      "            64,\n",
      "            64\n",
      "        ]\n",
      "    },\n",
      "    \"actor_critic\":\t\"mlp_actor_critic\",\n",
      "    \"algo\":\t\"trpo\",\n",
      "    \"backtrack_coeff\":\t0.8,\n",
      "    \"backtrack_iters\":\t10,\n",
      "    \"cg_iters\":\t10,\n",
      "    \"damping_coeff\":\t0.1,\n",
      "    \"delta\":\t0.01,\n",
      "    \"env_fn\":\t\"<function <lambda> at 0x7ffa47a7cfe0>\",\n",
      "    \"epochs\":\t50,\n",
      "    \"exp_name\":\t\"trpo\",\n",
      "    \"gamma\":\t0.99,\n",
      "    \"lam\":\t0.97,\n",
      "    \"logger\":\t{\n",
      "        \"<spinup.utils.logx.EpochLogger object at 0x7ffa46f3b090>\":\t{\n",
      "            \"epoch_dict\":\t{},\n",
      "            \"exp_name\":\t\"trpo\",\n",
      "            \"first_row\":\ttrue,\n",
      "            \"log_current_row\":\t{},\n",
      "            \"log_headers\":\t[],\n",
      "            \"output_dir\":\t\"/home/mike_1102/spinningup/spinningup/data/trpo/trpo_s0\",\n",
      "            \"output_file\":\t{\n",
      "                \"<_io.TextIOWrapper name='/home/mike_1102/spinningup/spinningup/data/trpo/trpo_s0/progress.txt' mode='w' encoding='UTF-8'>\":\t{\n",
      "                    \"mode\":\t\"w\"\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    \"logger_kwargs\":\t{\n",
      "        \"exp_name\":\t\"trpo\",\n",
      "        \"output_dir\":\t\"/home/mike_1102/spinningup/spinningup/data/trpo/trpo_s0\"\n",
      "    },\n",
      "    \"max_ep_len\":\t1000,\n",
      "    \"save_freq\":\t10,\n",
      "    \"seed\":\t0,\n",
      "    \"steps_per_epoch\":\t4000,\n",
      "    \"train_v_iters\":\t80,\n",
      "    \"vf_lr\":\t0.001\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike_1102/miniconda3/envs/env02/lib/python3.11/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment HalfCheetah-v2 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "DependencyNotInstalled",
     "evalue": "No module named 'mujoco_py'. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspinup\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_logger_kwargs\n\u001b[1;32m     22\u001b[0m logger_kwargs \u001b[38;5;241m=\u001b[39m setup_logger_kwargs(args\u001b[38;5;241m.\u001b[39mexp_name, args\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m---> 24\u001b[0m trpo(\u001b[38;5;28;01mlambda\u001b[39;00m : gym\u001b[38;5;241m.\u001b[39mmake(args\u001b[38;5;241m.\u001b[39menv), actor_critic\u001b[38;5;241m=\u001b[39mcore\u001b[38;5;241m.\u001b[39mmlp_actor_critic,\n\u001b[1;32m     25\u001b[0m \t\tac_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(hidden_sizes\u001b[38;5;241m=\u001b[39m[args\u001b[38;5;241m.\u001b[39mhid]\u001b[38;5;241m*\u001b[39margs\u001b[38;5;241m.\u001b[39ml), gamma\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mgamma, \n\u001b[1;32m     26\u001b[0m \t\tseed\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mseed, steps_per_epoch\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39msteps, epochs\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mepochs,\n\u001b[1;32m     27\u001b[0m \t\tlogger_kwargs\u001b[38;5;241m=\u001b[39mlogger_kwargs)\n",
      "Cell \u001b[0;32mIn[12], line 115\u001b[0m, in \u001b[0;36mtrpo\u001b[0;34m(env_fn, actor_critic, ac_kwargs, seed, steps_per_epoch, epochs, gamma, delta, vf_lr, train_v_iters, damping_coeff, cg_iters, backtrack_iters, backtrack_coeff, lam, max_ep_len, logger_kwargs, save_freq, algo)\u001b[0m\n\u001b[1;32m    112\u001b[0m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mset_seed(seed)\n\u001b[1;32m    113\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[0;32m--> 115\u001b[0m env \u001b[38;5;241m=\u001b[39m env_fn()\n\u001b[1;32m    116\u001b[0m obs_dim \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    117\u001b[0m act_dim \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn[13], line 24\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspinup\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_logger_kwargs\n\u001b[1;32m     22\u001b[0m logger_kwargs \u001b[38;5;241m=\u001b[39m setup_logger_kwargs(args\u001b[38;5;241m.\u001b[39mexp_name, args\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m---> 24\u001b[0m trpo(\u001b[38;5;28;01mlambda\u001b[39;00m : gym\u001b[38;5;241m.\u001b[39mmake(args\u001b[38;5;241m.\u001b[39menv), actor_critic\u001b[38;5;241m=\u001b[39mcore\u001b[38;5;241m.\u001b[39mmlp_actor_critic,\n\u001b[1;32m     25\u001b[0m \t\tac_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(hidden_sizes\u001b[38;5;241m=\u001b[39m[args\u001b[38;5;241m.\u001b[39mhid]\u001b[38;5;241m*\u001b[39margs\u001b[38;5;241m.\u001b[39ml), gamma\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mgamma, \n\u001b[1;32m     26\u001b[0m \t\tseed\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mseed, steps_per_epoch\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39msteps, epochs\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mepochs,\n\u001b[1;32m     27\u001b[0m \t\tlogger_kwargs\u001b[38;5;241m=\u001b[39mlogger_kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/env02/lib/python3.11/site-packages/gym/envs/registration.py:640\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m     render_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 640\u001b[0m     env \u001b[38;5;241m=\u001b[39m env_creator(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    643\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot an unexpected keyword argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrender_mode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m apply_human_rendering\n\u001b[1;32m    645\u001b[0m     ):\n",
      "File \u001b[0;32m~/miniconda3/envs/env02/lib/python3.11/site-packages/gym/envs/mujoco/half_cheetah.py:20\u001b[0m, in \u001b[0;36mHalfCheetahEnv.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     19\u001b[0m     observation_space \u001b[38;5;241m=\u001b[39m Box(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, high\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m17\u001b[39m,), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m---> 20\u001b[0m     MuJocoPyEnv\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhalf_cheetah.xml\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m5\u001b[39m, observation_space\u001b[38;5;241m=\u001b[39mobservation_space, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     23\u001b[0m     utils\u001b[38;5;241m.\u001b[39mEzPickle\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/env02/lib/python3.11/site-packages/gym/envs/mujoco/mujoco_env.py:186\u001b[0m, in \u001b[0;36mMuJocoPyEnv.__init__\u001b[0;34m(self, model_path, frame_skip, observation_space, render_mode, width, height, camera_id, camera_name)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    176\u001b[0m     model_path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m     camera_name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    184\u001b[0m ):\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m MUJOCO_PY_IMPORT_ERROR \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mDependencyNotInstalled(\n\u001b[1;32m    187\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMUJOCO_PY_IMPORT_ERROR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    188\u001b[0m         )\n\u001b[1;32m    190\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis version of the mujoco environments depends \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon the mujoco-py bindings, which are no longer maintained \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou are trying to precisely replicate previous works).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m     )\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    199\u001b[0m         model_path,\n\u001b[1;32m    200\u001b[0m         frame_skip,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m         camera_name,\n\u001b[1;32m    207\u001b[0m     )\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m: No module named 'mujoco_py'. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--env', type=str, default='HalfCheetah-v2')\n",
    "parser.add_argument('--hid', type=int, default=64)\n",
    "parser.add_argument('--l', type=int, default=2)\n",
    "parser.add_argument('--gamma', type=float, default=0.99)\n",
    "parser.add_argument('--seed', '-s', type=int, default=0)\n",
    "parser.add_argument('--cpu', type=int, default=4)\n",
    "parser.add_argument('--steps', type=int, default=4000)\n",
    "parser.add_argument('--epochs', type=int, default=50)\n",
    "parser.add_argument('--exp_name', type=str, default='trpo')\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "#args = parser.parse_args([arg for arg in sys.argv[1:] if arg.startswith(\"--\")])\n",
    "#args = parser.parse_args()\n",
    "\n",
    "#mpi_fork(args.cpu)  # run parallel code with mpi\n",
    "\n",
    "from spinup.utils.run_utils import setup_logger_kwargs\n",
    "logger_kwargs = setup_logger_kwargs(args.exp_name, args.seed)\n",
    "\n",
    "trpo(lambda : gym.make(args.env), actor_critic=core.mlp_actor_critic,\n",
    "\t\tac_kwargs=dict(hidden_sizes=[args.hid]*args.l), gamma=args.gamma, \n",
    "\t\tseed=args.seed, steps_per_epoch=args.steps, epochs=args.epochs,\n",
    "\t\tlogger_kwargs=logger_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
